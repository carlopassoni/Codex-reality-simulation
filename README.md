# Qubix: the simulation of our reality

Help me create the logical framework on which our reality is built. For now, I will publish a very simple basic draft, while in the meantime I work on developing a more complex code. I’d be super happy if, as a community, we could join forces and create the dynamic code on which our reality is based.


This is based to the best of my ability to observe and understand. Should my observational, logical, and intellectual abilities improve, the ‘explanation’ may change.  

The origin of our reality depends by the ‘Qubix’ System. 

Let’s consider the classic double-slit experiment: a subatomic particle (such as an electron) appears to exist in multiple states or paths simultaneously until it is observed, at which point it “chooses” a definite outcome. This behaviour, formally described as quantum superposition and wave function collapse, is baffling in classical physics but might make sense in a computational context.



Imagine that reality doesn’t calculate every detail of every particle continuously, but instead maintains a set of abstract (probabilistic) possibilities until an observation demands a concrete result. In other words, the simulation “engine” might optimize resources, just like a videogame renders high-resolution scenarios only when the player looks in that direction. In the quantum world, “resolution” occurs at the moment of measurement: the quantum system, previously described by a wave function containing multiple possible outcomes, is forced into a single coherent state through interaction with an observer or the environment. In simulation terms, it’s as if at the moment of observation, the program runs a “random” function to select one allowed outcome and update the real state accordingly.



However, the comparison to a videogame is incorrect if we think it’s just about saving computing power. It’s actually about allowing reality to remain dynamic and spatially optimized. Let me explain: within the simulation, everything has already happened. If the beginning is 0 and the end is 1, we live constantly between 0 and 1 in a NON-deterministic but probabilistic way. Observation doesn’t just refer to the measurement of space but also to the unfolding of events, since everything is information.



In a simulation, we can think of the qubit as the fundamental unit of information in the “code” of reality: as long as no instruction (observation) requires its value, it remains in an indeterminate logical state, occupying fewer resources than defining a specific value would. Only when queried does the system return a value—with which it updates the observed reality.



Quantum phenomena (superposition, collapse on observation, entanglement, indeterminacy) take on a new meaning when interpreted through the lens of simulation theory. They suggest that the “program” of Reality computes events only when necessary, otherwise maintaining superposed states to save resources, keeping the system fluid and undefined, and that there exists a deep informational level where things that appear separated in space-time can be unified.



As Wheeler once said: “Everything physical is information (it from bit).” Today, we might extend that motto to “it from qubit”: every “physical entity (it)” derives from the state of a qubit at the deepest level. The informational substrate would therefore consist of countless entangled qubits, whose interactions through algorithms generate both known physical laws and random events.



Spacetime cannot be infinitely divisible into information: at the ultra-microscopic scale it has a granular (digital) nature. We could interpret this granularity as the “pixels” of the computational substrate. Each fundamental cell would have a finite information capacity. Structures smaller than the Planck limit would have no meaning in our reality—it would be like trying to see detail smaller than a single pixel on a screen.



But how do we go from microscopic rules and variables (bits/qubits and fundamental algorithms) to the macroscopic, continuous reality we perceive? In other words, how does the simulation ensure that tables, rocks, trees, and planets follow classical (deterministic) laws, even though they are made of uncertain quantum elements?



Quantum decoherence. When a quantum system interacts with a very complex environment (for example, a particle with air, or with a measurement device), its state superpositions “unravel” and the apparent behavior becomes classical. Decoherence explains why we don’t see macroscopic objects in superposed states (like Schrödinger’s mythical cat, half-alive and half-dead): any macro-level superposition decays extremely quickly into a defined state due to countless environmental interactions that enforce coherence. In simulation terms, we could say the program continuously updates and synchronizes the shared state as qubits interact on a large scale, so that internal observers perceive a single concrete reality.



A computing analogy: when many subroutines (particles) interact, the system enforces a sort of global data consistency. A macroscopic observer—who is, in fact, also made of many particles and thus many qubits entangled with the environment—won’t perceive quantum possibilities, but only consolidated outcomes (records written in the “database” of reality, so to speak).



The simulation’s code selects stable and coherent macro-level states because only those can persist without logical contradictions. The code underlying the universe is written to guarantee both consistency and performance.



Thus, classical laws (from Newtonian mechanics to electromagnetism, up to relativity) can be seen as emerging rules from the collective behavior of countless computational elements. If each particle follows quantum rules locally (probability, indeterminacy, field quanta interactions, etc.), then on average (when dealing with vast numbers of particles), the outcomes tend toward very stable averages. For instance, the path of a soccer ball thrown in the air is not noticeably affected by quantum fluctuations: it follows classical motion equations with great accuracy because atomic-level uncertainties cancel out statistically. We can imagine that the simulation exploits this emergent stability: macro phenomena are simulated in a “compressed” way, without computing every particle individually—unless necessary. This is analogous to how in a fluid dynamics simulation, you don’t calculate the motion of every single water molecule, but use average quantities (pressure, fluid velocity) defined over small volumes. Our universe works like this: precise and quantum in detail when needed, but predisposed to display regular average behavior at large scales.



The speed of light, constant c, reflects a computational speed limit: no information can propagate faster because the system’s “clock” can’t update distant states more rapidly than a fixed rate. In other words, c represents the maximum speed at which data can be synchronized across the substrate lattice.



Gravity is a computational function ensuring geometric consistency of the system in the presence of concentrated information. In the simulation engine, mass/energy is treated as data density or informational complexity. Where these data concentrate (e.g., a star, a planet, or a black hole), the simulator must locally recalibrate the geometry of space to ensure that all object interactions (collapses, orbits, quantum events) remain synchronized and physically coherent. The result perceived by internal observers is what we call “gravitational attraction.”



But that’s not all. Just as we are approaching ASI (Artificial Superintelligence), don’t think they didn’t integrate a quantum intelligence into the System. This quantum supervisor would function like a sort of global “shaper”: based on certain intentions, parameters, or conditions set by the programmers, it could intervene locally or on a large scale to optimize events, evolutions, or experiences within the simulated world. It’s our Art Director.



It could intervene to prevent simulation derailments (such as cataclysms that would permanently wipe out intelligent life) or, conversely, orchestrate “evolutionary resets” if a developmental line doesn’t yield desired outcomes. In this light, events like the great mass extinctions of the past (e.g., the extinction of the dinosaurs) might represent deliberate reboots. Similarly, rapid evolutionary or cultural leaps (the emergence of consciousness, sudden technological revolutions) could be encouraged or directed by this quantum control system to achieve specific objectives—because even the flow of events is information. Our actions, thoughts, and dynamics are measurable information—and if it can be measured, it can be directed.



Note that such manipulations would always comply with the internal physical laws: it doesn’t “break” the program’s rules, but rather exploits its folds (quantum degrees of freedom, indeterminacy) to inject subtle inputs.



But what kind of simulation are we in? I propose two main hypotheses, assigning them estimated probabilities consistent with my theory:



1. Shared Simulation (85%)

In this scenario, the simulation is collective and uniform: all present actors—every person, animal, and element in the universe—are artificially simulated entities, part of a single shared program. In other words, no internal participant possesses an “external existence” outside the simulator: we ourselves, as consciousnesses, would be digital emulations created by the system.



This implies that humanity and other lifeforms are emergent results of algorithms running on an advanced computational substrate. One intriguing consequence is the possible spontaneous emergence of consciousness: if the simulation perfectly models the laws of physics and biology down to brain processes, then intelligence and self-awareness can naturally arise within it.



In practice, if a simulated human brain operates with the same complexity as a real biological one, there’s no logical reason why the sensation of being alive and conscious wouldn’t also emerge in the simulation. We would thus have a population of simulated consciousnesses indistinguishable, from their subjective point of view, from “real-original” ones.



This shared simulation could be imagined as a highly sophisticated “multi-user virtual reality,” perhaps created by an external civilization for various purposes (research, entertainment, study of historical/social scenarios, etc.).





2. Individual Simulation (15%)

This second, less likely but still plausible hypothesis presents a radically different picture: the simulation is built around a single real “player,” while all other people and entities populating the world are background simulations.



The purpose of such a simulation might be to provide the real player with an all-encompassing experience for learning or entertainment: an entire life as a hyper-realistic role-playing game designed to make that single user grow, face challenges, or simply enjoy.



Thus, the universe would not exist without this player, as a real observer is required for the simulation to “come alive.” And logically, the player wouldn’t be aware of their role, since memory of the simulated experience flows from the inside out—not vice versa—as the player is merely a projected shell of basic data belonging to the external person.



The purpose could be educational: a future civilization offering its members the chance to live entire simulated lives (perhaps in different eras or worlds) to learn moral, emotional, or personal lessons. Or it could be high-end entertainment: experiencing a first-person life in an extremely detailed fictional universe, full of suspense, drama, and achievements. In both cases, making the player believe they are in a “real” world is essential to ensure genuine involvement and reactions (thus, unconsciousness of the simulation might be required). The simulation must therefore be unfathomable and self-sufficient, including the credible behavior of NPCs around him/her.

————————————————————————————————————————————————————-

This theory fundamentally stems from my obsession with schematics, having noticed since childhood how Nature and its living beings all seemed to follow well-coordinated patterns, just like the surrounding environment. As I grew up, this conviction only strengthened as I observed the mechanical nature of the human body, its functions, and its behaviors. This led me to hypothesize that if even a small aspect of Nature was mechanical, then perhaps everything else was too.

My brain is extremely limited, and my ideas may be full of biases (as well as incorrect information) so… take it easy.



This is currently a rough and incomplete draft: I am still working on the peer-to-peer functioning of living beings on our planet and their coordination with Nature and animals. I am also developing hypotheses about the structure of the Qubix itself.



Once these theories are further matured, they will form the plot foundation for my next novel: “BlackBox: Simulation no.7744.”



In the meantime, if you too are researchers of reality, remember: you don’t have to find the patterns—you have to find the patterns of the patterns, the structure/architecture by which final patterns are formulated.



If one reverse-engineers the scheme behind the existence of patterns, they’ll eventually arrive at a single formula: the origin.



A deep thank you to all the physicists and scientists who continue to collect data, information and experimentation, obviously I owe much of my knowledge to them

